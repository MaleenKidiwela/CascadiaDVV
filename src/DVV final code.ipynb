{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabd4a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "using SeisNoise, SeisIO, Plots\n",
    "using Dates \n",
    "using Plots\n",
    "using SeisDvv\n",
    "using CSV\n",
    "using Statistics\n",
    "using LinearAlgebra\n",
    "using Deconvolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01f426e",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ncf_denoise function\n",
    "function ncf_denoise_weiner(img_to_denoise::Matrix{Float32}, mdate::Int64, ntau::Int64, nsv::Int64, nsv_to_rm::Int64, use_wiener::Bool, noise_power::Float64)\n",
    "    m, n = size(img_to_denoise)\n",
    "    nsv = min(nsv, m, n)\n",
    "\n",
    "    U, s, V = svd(img_to_denoise)\n",
    "    Xwiener = zeros(Float32, size(img_to_denoise))  # Ensure the type is Float32\n",
    "\n",
    "    for kk in (nsv_to_rm + 1):nsv\n",
    "        SV = Diagonal([kk == i ? s[kk] : 0 for i in 1:min(m, n)])\n",
    "\n",
    "        X = U * SV * V'\n",
    "\n",
    "        if use_wiener\n",
    "            # Convert X to Float64 if necessary\n",
    "            X64 = convert(Matrix{Float64}, X)\n",
    "            Xwiener += wiener(X64, X64, noise_power)\n",
    "        else\n",
    "            Xwiener += X\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if use_wiener\n",
    "        # Final Wiener filter application, converting Xwiener to Float64\n",
    "        Xwiener64 = convert(Matrix{Float64}, Xwiener)\n",
    "        return wiener(Xwiener64, Xwiener64, noise_power)\n",
    "    else\n",
    "        return Xwiener\n",
    "    end\n",
    "end\n",
    "\n",
    "# Convert UNIX timestamp to DateTime\n",
    "unixtimestamp_to_datetime(ts) = unix2datetime(ts)\n",
    "\n",
    "function find_indices_in_window(start_times, window_start, window_end)\n",
    "    return findall(t -> window_start <= unixtimestamp_to_datetime(t) <= window_end, start_times)\n",
    "end\n",
    "\n",
    "function stack_moving_window(corr_data, window_size_days, overlap_days)\n",
    "    window_size = Day(window_size_days)\n",
    "    overlap = Day(overlap_days)\n",
    "\n",
    "    # Initialize a matrix for the stacked correlations and a vector for the start times\n",
    "    stacked_correlations_matrix = Array{Float32}(undef, 8001, 0)\n",
    "    stacked_start_times = Float64[]\n",
    "\n",
    "    current_start_date = unixtimestamp_to_datetime(first(corr_data.t))\n",
    "\n",
    "    while current_start_date <= unixtimestamp_to_datetime(last(corr_data.t))\n",
    "        current_end_date = current_start_date + window_size - Day(1)\n",
    "        indices = find_indices_in_window(corr_data.t, current_start_date, current_end_date)\n",
    "\n",
    "        if length(indices) > window_size_days/10#4#2.8 #3\n",
    "            window_data = deepcopy(corr_data)\n",
    "            window_data.corr = corr_data.corr[:, indices]\n",
    "            stacked_corr = stack(window_data, allstack=true).corr\n",
    "\n",
    "            # Append the stacked correlation as a new column\n",
    "            stacked_correlations_matrix = hcat(stacked_correlations_matrix, stacked_corr)\n",
    "\n",
    "            # Append the start time of the window\n",
    "            push!(stacked_start_times, datetime2unix(current_start_date))\n",
    "        end\n",
    "\n",
    "        current_start_date += overlap\n",
    "    end\n",
    "\n",
    "    # Create a new CorrData instance with the stacked data\n",
    "    new_corr_data = deepcopy(corr_data)\n",
    "    new_corr_data.corr = stacked_correlations_matrix\n",
    "    new_corr_data.t = datetime2unix.(unix2datetime.(stacked_start_times) .+ Day(window_size_days/2))\n",
    "\n",
    "    return new_corr_data\n",
    "end\n",
    "\n",
    "# Averaging function for causal and acausal parts\n",
    "function average_causal_acausal(corr, time)\n",
    "    averaged_corr = copy(corr)\n",
    "    causal_index = findfirst(time .>= 0)\n",
    "    acausal_index = findlast(time .< 0)\n",
    "\n",
    "    for k in 1:causal_index-1\n",
    "        if acausal_index - k + 1 > 0\n",
    "            averaged_corr[causal_index + k - 1] = (averaged_corr[acausal_index - k + 1] + averaged_corr[causal_index + k - 1]) / 2\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return averaged_corr\n",
    "end\n",
    "\n",
    "# Function to create an evenly spaced array\n",
    "function evenly_spaced(a, b, n)\n",
    "    h = (b - a) / (n - 1)\n",
    "    collect(a:h:b)\n",
    "end\n",
    "\n",
    "function update_progress_bar(current_step::Int, start_step::Int, end_step::Int)\n",
    "    total_steps = end_step - start_step\n",
    "    progress = (current_step - start_step) / total_steps\n",
    "    filled_length = round(Int, 50 * progress)\n",
    "    bar = repeat('█', filled_length) * repeat(' ', 50 - filled_length)\n",
    "    print(\"\\rProgress: |$bar| $(round(progress * 100))% Complete\")\n",
    "end\n",
    "\n",
    "function ncf_denoise(img_to_denoise::Matrix{Float32}, mdate::Int64, ntau::Int64, nsv::Int64, nsv_to_rm::Int64, use_wiener::Bool, noise_power::Float64)\n",
    "    m, n = size(img_to_denoise)\n",
    "    nsv = min(nsv, m, n)  # Ensure nsv does not exceed the dimensions of the image\n",
    "\n",
    "    U, s, V = svd(img_to_denoise)  # Perform singular value decomposition\n",
    "    Xwiener = zeros(Float32, m, n)  # Initialize the result matrix of type Float32\n",
    "    \n",
    "    total_steps = nsv - nsv_to_rm  # Calculate total steps required for the loop\n",
    "    println(\"Starting denoising...\")  # Initial message before processing starts\n",
    "\n",
    "    for kk in (nsv_to_rm + 1):nsv\n",
    "        # Perform rank-1 updates using only significant singular values\n",
    "        Xwiener .+= U[:, kk] * s[kk] * V[:, kk]'  # Scale outer product by the singular value\n",
    "        \n",
    "        # Progress bar update\n",
    "        progress = (kk - nsv_to_rm) / total_steps  # Calculate progress ratio\n",
    "        filled_length = round(Int, 50 * progress)  # Calculate number of blocks to fill\n",
    "        bar = repeat('█', filled_length) * repeat(' ', 50 - filled_length)  # Create progress bar\n",
    "        print(\"\\rProgress: |$bar| $(round(progress * 100))% Complete\")  # Print progress bar\n",
    "    end\n",
    "    \n",
    "    println(\"\\nDenoising completed.\")  # End message after processing completes\n",
    "\n",
    "    return Xwiener  # Return the reconstructed, denoised image\n",
    "end\n",
    "\n",
    "function plot_corr_heatmap(C::CorrData)\n",
    "    # Define lags in seconds\n",
    "    lags = -C.maxlag : (1 / C.fs) : C.maxlag\n",
    "    \n",
    "    # Convert each Unix timestamp to DateTime and shift by +20 days\n",
    "    datetimes_shifted = unix2datetime.(C.t) #.+ Day(20)\n",
    "    \n",
    "    # Format them to strings without time part\n",
    "    times_str = Dates.format.(datetimes_shifted, \"yyyy/m/d\")\n",
    "    \n",
    "    # Numeric y-values for each row in `C.corr'`\n",
    "    yvals = 1:length(times_str)\n",
    "    \n",
    "    # Choose fewer y-tick positions to avoid a crowded axis\n",
    "    # For instance, one label every 100 rows:\n",
    "    tick_skip   = 100\n",
    "    ytick_vals  = 1:tick_skip:length(yvals)\n",
    "    ytick_labels = times_str[ytick_vals]\n",
    "\n",
    "    # Create the heatmap with numeric y-values\n",
    "    p = heatmap(\n",
    "        lags,\n",
    "        yvals,\n",
    "        C.corr',\n",
    "        size           = (1000, 500),\n",
    "        left_margin    = 20Plots.mm,\n",
    "        right_margin   = 5Plots.mm,\n",
    "        bottom_margin  = 10Plots.mm,\n",
    "        dpi            = 200,\n",
    "        seriescolor    = :balance,\n",
    "        ytickfontsize  = 10,\n",
    "        xtickfontsize  = 8,\n",
    "        xlim           = [-10, 10],\n",
    "        grid           = :true,\n",
    "        legend         = :false,\n",
    "        gridalpha      = 1.0,\n",
    "        gridlinewidth  = 2,\n",
    "        gridcolor      = :black,\n",
    "        framestyle     = :grid,\n",
    "        # Only place ticks at y=ytick_vals, labeling them with shifted dates\n",
    "        yticks         = (ytick_vals, ytick_labels),\n",
    "    )\n",
    "\n",
    "    # Add vertical lines at x = -4, -3, -2, -1, 0, 1, 2, 3, 4\n",
    "    vlines = [-4, -3, -2, -1, 0, 1, 2, 3, 4]\n",
    "    for x_val in vlines\n",
    "        vline!([x_val], color = :black, linewidth = 1)\n",
    "    end\n",
    "\n",
    "    # Add horizontal lines every 100 rows\n",
    "    for i in 1:100:length(yvals)\n",
    "        hline!([i], color = :black, linewidth = 1)\n",
    "    end\n",
    "\n",
    "    return p\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e777e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_a = \"BHZ\"\n",
    "comp_b = \"BHE\"\n",
    "suffix = \"ZE\"\n",
    "station = \"HYSB1\"\n",
    "network = \"OO\"\n",
    "\n",
    "mnf = 1.0\n",
    "mxf = 2.0\n",
    "ndaystack = 40\n",
    "xlim_corr = [-40,40]\n",
    "\n",
    "# Denoise allstk and dailystk and store in the corrdata\n",
    "mdate = 10\n",
    "ntau = 10\n",
    "nsv =7\n",
    "nsv_to_rm = 0\n",
    "use_wiener = false\n",
    "exclude_dates = true #false #\n",
    "\n",
    "# Estimate of the noise power \n",
    "noise_power = 0.03  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8104ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = load_corr(\"/data/wsd02/maleen_data/C_test/$network.$station..$comp_a.$network.$station..$comp_b.jld2\", \"$suffix\")\n",
    "#creating 1 day stacks\n",
    "ndaystk = 1\n",
    "d=stack(dd,interval=Day(ndaystk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3d68c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if exclude_dates\n",
    "    # Convert d.t to DateTime objects\n",
    "    dt_dates = [unix2datetime(ts) for ts in d.t];\n",
    "\n",
    "    # Define the date ranges to exclude\n",
    "    #exclude_ranges = [(DateTime(2016,8,15), DateTime(2017,2,21)),(DateTime(2017,3,5), DateTime(2017,7,14))]\n",
    "    #exclude_ranges = [(DateTime(2013,1,1), DateTime(2015,1,1))];\n",
    "    exclude_ranges = [(DateTime(2013,1,1), DateTime(2015,1,1)),(DateTime(2018,1,1), DateTime(2021,1,1))];\n",
    "    #exclude_ranges = [(DateTime(2013,1,1), DateTime(2021,8,1))]#, (DateTime(2022,12,15), DateTime(2023,1,5))];\n",
    "    #exclude_ranges = [(DateTime(2022,12,15), DateTime(2023,1,5))];\n",
    "    #exclude_ranges = [(DateTime(2016,8,15), DateTime(2017,8,14))]\n",
    "    #exclude_ranges = [(DateTime(2016,8,15), DateTime(2017,2,16)),(DateTime(2017,3,12), DateTime(2017,8,14)),(DateTime(2014,11,1), DateTime(2014,11,20),(DateTime(2020,6,15), DateTime(2021,8,1)))]\n",
    "    #exclude_ranges = [(DateTime(2020,8,14), DateTime(2021,9,1)),(DateTime(2014,11,1), DateTime(2015,1,1))]\n",
    "\n",
    "    # Find indices of dates within the specified ranges\n",
    "    indices_to_remove = findall(dt -> any(start_date <= dt <= end_date for (start_date, end_date) in exclude_ranges), dt_dates);\n",
    "    d.t = deleteat!(d.t, indices_to_remove);\n",
    "    # Identify all column indices\n",
    "    total_columns = size(d.corr, 2);\n",
    "    all_indices = 1:total_columns;\n",
    "\n",
    "    # Determine the indices of columns to keep\n",
    "    indices_to_keep = setdiff(all_indices, indices_to_remove);\n",
    "\n",
    "    # Update t.corr by keeping only the desired columns\n",
    "    d.corr = d.corr[:, indices_to_keep];\n",
    "    println(\"dates excluded\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8620cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess your data\n",
    "remove_nan!(d)\n",
    "#dailystk =d #test\n",
    "dailystk = stack_moving_window(d, ndaystack, 1)\n",
    "clean_up!(dailystk, mnf, mxf)\n",
    "abs_max!(dailystk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4eca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dailystk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa833c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot(dailystk,size=(1000, 500),left_margin=20Plots.mm, right_margin=5Plots.mm, bottom_margin=10Plots.mm, dpi=200,xlim=xlim_corr)\n",
    "# Example usage (assuming `dailystk` is your CorrData object):\n",
    "clean_up!(d, mnf, mxf)\n",
    "abs_max!(d)\n",
    "lp = plot_corr_heatmap(d)\n",
    "display(lp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b7ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = evenly_spaced(-100, 100, size(dailystk.corr, 1))\n",
    "max_lag=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e37131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#allstk.corr = ncf_denoise(allstk.corr, mdate, ntau, nsv, nsv_to_rm, use_wiener, noise_power);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188253a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dailystk.corr = ncf_denoise(dailystk.corr, mdate, ntau, nsv, nsv_to_rm, use_wiener, noise_power);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb2a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "allstk = stack(dailystk, allstack=true)\n",
    "clean_up!(allstk, mnf, mxf)\n",
    "abs_max!(allstk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6b2e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (assuming `dailystk` is your CorrData object):\n",
    "ppp = plot_corr_heatmap(dailystk)\n",
    "display(ppp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f176fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dailystk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be792323",
   "metadata": {},
   "source": [
    "## Parallelized Stretching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5e6ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributed\n",
    "\n",
    "# Add processes (if not already added)\n",
    "if nprocs() == 1  # Only the main process is running\n",
    "    addprocs(36)  # Adjust the number as needed\n",
    "end\n",
    "\n",
    "@everywhere begin\n",
    "    using SharedArrays, HDF5, Plots, SeisNoise, SeisIO, Plots\n",
    "    using Dates \n",
    "    using Plots\n",
    "    using SeisDvv\n",
    "    # Ensure all necessary modules are loaded in each worker\n",
    "\n",
    "    fs = 40.0     # sample frequency\n",
    "    win_len = 2.0  # sliding window length\n",
    "    win_step = 5.0 # sliding window step\n",
    "    tmin = -10.0\n",
    "    tmax = -2.0\n",
    "    dmn = -0.1 #05\n",
    "    dmx = 0.1 #05\n",
    "end\n",
    "\n",
    "# Define your data or load it in a way that each worker can access it\n",
    "# Assuming dailystk.corr is accessible to all workers, or needs to be distributed\n",
    "j = size(dailystk.corr, 2)\n",
    "dvt = SharedArray{Float64}(j)\n",
    "cct = SharedArray{Float64}(j)\n",
    "cctb = SharedArray{Float64}(j)\n",
    "\n",
    "@sync @distributed for i = 1:j\n",
    "    S1 = Array(allstk.corr[:])\n",
    "    S2 = Array(dailystk.corr[:,i])\n",
    "    rel_tmin_index = Int(floor((tmin + 100+max_lag) * fs))\n",
    "    rel_tmax_index = Int(floor((tmax + 100+max_lag) * fs))\n",
    "    \n",
    "    window = collect(rel_tmin_index:rel_tmax_index)\n",
    "    fmin = mnf  # Ensure mnf is defined or loaded\n",
    "    fmax = mxf  # Ensure mxf is defined or loaded\n",
    "\n",
    "    function evenly_spaced(a, b, n)\n",
    "        h = (b - a) / (n - 1)\n",
    "        collect(a:h:b)\n",
    "    end\n",
    "\n",
    "    time = evenly_spaced(-100, 100, size(dailystk.corr, 1))\n",
    "    time = time.-max_lag\n",
    "\n",
    "    dvv_ts, cc_ts, cdp_Ts, eps_ts, err_ts, allC_ts = SeisDvv.stretching(S1, S2, time, window, fmin, fmax, dvmin=dmn, dvmax=dmx, ntrial=3000)\n",
    "    dvt[i] = dvv_ts\n",
    "    cct[i] = cc_ts\n",
    "    cctb[i] = cdp_Ts\n",
    "end\n",
    "\n",
    "println(\"Computation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42231576",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = map(unix2datetime, dailystk.t);\n",
    "# Assuming datetime_vector is your 71-element Vector{DateTime} \n",
    "# and float_vector is your 71-element Vector{Float64}\n",
    "datetime_vector = dt;  # fill this with your data\n",
    "float_vector = dvt;  # fill this with your data\n",
    "cctfilter = cct;\n",
    "\n",
    "# Combine the vectors into a named tuple which CSV.File will interpret as a table\n",
    "data = (DateTime=datetime_vector, Float=float_vector,cct);\n",
    "\n",
    "# Set the threshold value\n",
    "threshold = 0.1\n",
    "\n",
    "# Filter the datetime and float vectors based on the threshold\n",
    "filtered_indices = cct .>= threshold;\n",
    "datetime_vector = datetime_vector[filtered_indices];\n",
    "float_vector = float_vector[filtered_indices];\n",
    "cctfilter = cct[filtered_indices];\n",
    "\n",
    "# Calculate the Interquartile Range (IQR)\n",
    "Q1 = quantile(float_vector, 0.1)#20)\n",
    "Q3 = quantile(float_vector, 0.9)#80)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Calculate the lower and upper bounds to filter the outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter the vectors to remove the outliers\n",
    "dtf = DateTime[]\n",
    "dvtf = Float64[]\n",
    "cctf = Float64[]\n",
    "for i in 1:length(float_vector)\n",
    "    if float_vector[i] >= lower_bound && float_vector[i] <= upper_bound\n",
    "        push!(dtf, datetime_vector[i])\n",
    "        push!(dvtf, float_vector[i])\n",
    "        push!(cctf, cctfilter[i])\n",
    "    end\n",
    "end\n",
    "# Calculate the mean and standard deviation\n",
    "mean_value = mean(cct)\n",
    "std_deviation = std(dvtf)\n",
    "\n",
    "# Print the results\n",
    "println(\"Mean of the float vector: $mean_value\")\n",
    "println(\"Standard deviation of the float vector: $std_deviation\")\n",
    "\n",
    "# Sample datetime_vector and dvtf\n",
    "datetime_vector = dtf\n",
    "\n",
    "# Find the minimum and maximum dates in the datetime_vector\n",
    "min_date = minimum(datetime_vector)\n",
    "max_date = maximum(datetime_vector)\n",
    "\n",
    "# Generate all dates in the date rangeI'\n",
    "all_dates = collect(min_date:Day(ndaystack):max_date)\n",
    "\n",
    "# Find the dates that are in all_dates but not in datetime_vector (considering only the date part)\n",
    "missing_dates = setdiff(all_dates, datetime_vector)\n",
    "\n",
    "# Assuming dt, dvt, cct, and dailystk are defined\n",
    "#dt = map(unix2datetime, dailystk.t)\n",
    "ticks = dtf[1:100:end]\n",
    "tick = Dates.format.(ticks, \"yyyy-mm-dd\")\n",
    "\n",
    "# First subplot\n",
    "p1 = plot(scatter(dtf, dvtf), seriestype = :line, xticks = (ticks, tick), xrot = -30, ylabel=\"dv/v %\", legend=false)#,ylim=(-0.5, 0.5))\n",
    "plot!(dtf, dvtf, seriestype = :line, xticks = (ticks, tick), xrot = -30, legend=false)\n",
    "# For each missing date, add a vertical line to the plot\n",
    "#for date in missing_dates\n",
    "#    vline!(p1, [DateTime(date)], linecolor=:red, linewidth=2, linealpha=0.5, label=\"Missing Date\")\n",
    "#end\n",
    "\n",
    "\n",
    "# Second subplot\n",
    "p2 = plot(dtf, cctf, seriestype = :line, label = \"cc\", xticks = (ticks, tick), xrot = -30, left_margin = 10Plots.mm, ylabel=\"cc\", xlabel=\"Date\", legend=false)\n",
    "#hline!([0.8], label=\"Threshold\", width=2, color=:red) # Adding a horizontal line at y=0.8\n",
    "\n",
    "# Third subplot\n",
    "p3 = plot(dailystk,xlim=[-30,30], left_margin = 10Plots.mm)\n",
    "\n",
    "# The layout now specifies a grid of 3 rows and 1 column and gives the third plot even more vertical space compared to the first two.\n",
    "l = @layout([a{0.3h}; b{0.2h}; c{0.5h}])\n",
    "\n",
    "# Combine the plots\n",
    "final_plot = plot(p1, p2, p3, layout = l, size = (1100, 1000), left_margin = 20Plots.mm, right_margin = 5Plots.mm)\n",
    "\n",
    "# Save the plot\n",
    "#savefig(final_plot, \"$(d.name)_t$(tmin)-$(tmax)_freq$(mnf)-$(mxf)_stk$(ndaystack)SVD7s.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define multiple ranges to exclude ---\n",
    "ranges_to_exclude = [\n",
    "    (Date(\"2020-12-10\"), Date(\"2021-01-01\")),\n",
    "#    (Date(\"2023-05-10\"), Date(\"2024-05-01\")),\n",
    "#    (Date(\"2022-11-05\"), Date(\"2023-01-05\")),\n",
    "#   (Date(\"2020-11-30\"), Date(\"2021-01-17\")),\n",
    "#    (Date(\"2016-10-05\"), Date(\"2017-03-09\")),\n",
    "#    (Date(\"2017-05-03\"), Date(\"2017-09-10\")),\n",
    "    (Date(\"2014-01-01\"), Date(\"2015-01-05\")),\n",
    "#    (Date(\"2018-05-20\"), Date(\"2018-08-05\")),\n",
    "#    (Date(\"2020-12-01\"), Date(\"2021-10-01\")),\n",
    "    (Date(\"2020-07-10\"), Date(\"2020-10-01\"))\n",
    "]\n",
    "\n",
    "dvadd=0.0\n",
    "\n",
    "# 1. Build a mask for all excluded points\n",
    "#    For each (start, end) pair, mark dtf .>= start AND dtf .<= end\n",
    "excluded_mask = fill(false, length(dtf))  # start with all false\n",
    "for (start_date, end_date) in ranges_to_exclude\n",
    "    # Mark as true if within any of the exclusion ranges\n",
    "    excluded_mask .|= (dtf .>= start_date) .& (dtf .<= end_date)\n",
    "end\n",
    "\n",
    "# 2. Take the negation to get the \"keep\" mask\n",
    "mask = .!excluded_mask\n",
    "\n",
    "# 3. Filter the data\n",
    "filtered_dtf  = dtf[mask]\n",
    "filtered_dvtf = dvtf[mask]\n",
    "filtered_cctf = cctf[mask]\n",
    "\n",
    "# --- Proceed with the plotting as usual ---\n",
    "filtered_dtf_offset = filtered_dtf\n",
    "# Update ticks based on filtered data\n",
    "ticks = filtered_dtf_offset[1:100:end]\n",
    "tick  = Dates.format.(ticks, \"yyyy-mm-dd\")\n",
    "\n",
    "# Define a color gradient\n",
    "my_colormap = :viridis\n",
    "\n",
    "# Normalize cctf if needed (here we simply keep it as is)\n",
    "normalized_cctf = filtered_cctf\n",
    "\n",
    "# Add leading spaces to the color bar title to create more space\n",
    "colorbar_title_with_space = \"          cc\"\n",
    "\n",
    "# Create the plot\n",
    "p1 = scatter(\n",
    "    filtered_dtf_offset, filtered_dvtf.+dvadd,\n",
    "    zcolor = normalized_cctf,\n",
    "    color   = my_colormap,\n",
    "    markerstrokewidth = 0,\n",
    "    colorbar          = true,\n",
    "    colorbar_title    = colorbar_title_with_space,\n",
    "    seriestype        = :scatter,\n",
    "    legend            = false,\n",
    "    xticks            = (ticks, tick),\n",
    "    xrot              = -30,\n",
    "    ylabel            = \"dv/v %\",\n",
    "    size              = (1000, 300),\n",
    "    left_margin       = 20Plots.mm,\n",
    "    right_margin      = 10Plots.mm,\n",
    "    bottom_margin     = 10Plots.mm,\n",
    "    dpi               = 200\n",
    ")\n",
    "\n",
    "# Optionally add another layer\n",
    "plot!(\n",
    "    p1,\n",
    "    filtered_dtf_offset, filtered_dvtf.+dvadd,\n",
    "    legend       = false,\n",
    "    xticks       = (ticks, tick),\n",
    "    xrot         = -50,\n",
    "    size         = (1200, 350),\n",
    "    #ylim         = (-0.12,0.12),\n",
    "    left_margin  = 20Plots.mm,\n",
    "    right_margin = 10Plots.mm,\n",
    "    bottom_margin= 10Plots.mm,\n",
    "    dpi          = 200\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "display(p1)\n",
    "\n",
    "# Save the figure if needed\n",
    "#savefig(p1, \"HYS141.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298347ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, DataFrames\n",
    "\n",
    "# Assuming dvt and cct are already defined as 75-element Vector{Float64}\n",
    "df = DataFrame(T = filtered_dtf_offset, DVT = filtered_dvtf.+dvadd, CCT = filtered_cctf)\n",
    "#file_name= \"HYSB1_early_ZE_negative_1-3.csv\"\n",
    "file_name= \"HYSB1_slowslip_1-2n.csv\"\n",
    "# Save the DataFrame to a CSV file\n",
    "CSV.write(file_name, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755e0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
