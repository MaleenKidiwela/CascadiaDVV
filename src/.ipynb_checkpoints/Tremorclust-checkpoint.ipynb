{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1076fda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.patheffects as PathEffects\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "os.makedirs('maps', exist_ok=True)\n",
    "os.makedirs('clustering_results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8ab2ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load tremor event data from CSV file and convert starttime to datetime.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the tremor events CSV file\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Loaded tremor data\n",
    "    \"\"\"\n",
    "    print(f\"Loading tremor events data from {file_path}...\")\n",
    "    tremor_data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert starttime to datetime\n",
    "    tremor_data['starttime'] = pd.to_datetime(tremor_data['starttime'])\n",
    "    \n",
    "    return tremor_data\n",
    "\n",
    "def filter_data(tremor_data, lat_min=44.4, lat_max=44.6):\n",
    "    \"\"\"\n",
    "    Filter tremor data based on latitude range.\n",
    "    \n",
    "    Args:\n",
    "        tremor_data (pandas.DataFrame): Tremor event data\n",
    "        lat_min (float): Minimum latitude\n",
    "        lat_max (float): Maximum latitude\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Filtered tremor data\n",
    "    \"\"\"\n",
    "    #print(f\"Filtering events between latitudes {lat_min} and {lat_max}...\")\n",
    "    filtered_data = tremor_data[(tremor_data['lat'] >= lat_min) & (tremor_data['lat'] <= lat_max)]\n",
    "    filtered_data = filtered_data.sort_values('starttime')\n",
    "    #print(f\"Found {len(filtered_data)} events between latitudes {lat_min} and {lat_max}\")\n",
    "    \n",
    "    # Save filtered data\n",
    "    filtered_data.to_csv('filtered_tremor_data.csv', index=False)\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate distance between two points using Haversine formula.\n",
    "    \n",
    "    Args:\n",
    "        lat1, lon1 (float): Latitude and longitude of first point\n",
    "        lat2, lon2 (float): Latitude and longitude of second point\n",
    "        \n",
    "    Returns:\n",
    "        float: Distance in kilometers\n",
    "    \"\"\"\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lon1_rad = math.radians(lon1)\n",
    "    lat2_rad = math.radians(lat2)\n",
    "    lon2_rad = math.radians(lon2)\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    # Radius of Earth in kilometers\n",
    "    radius = 6371.0\n",
    "    distance = radius * c\n",
    "    \n",
    "    return distance\n",
    "\n",
    "def calculate_bearing(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate bearing between two points.\n",
    "    \n",
    "    Args:\n",
    "        lat1, lon1 (float): Latitude and longitude of first point\n",
    "        lat2, lon2 (float): Latitude and longitude of second point\n",
    "        \n",
    "    Returns:\n",
    "        float: Bearing in degrees (0-360)\n",
    "    \"\"\"\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lon1_rad = math.radians(lon1)\n",
    "    lat2_rad = math.radians(lat2)\n",
    "    lon2_rad = math.radians(lon2)\n",
    "    \n",
    "    # Calculate bearing\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    y = math.sin(dlon) * math.cos(lat2_rad)\n",
    "    x = math.cos(lat1_rad) * math.sin(lat2_rad) - math.sin(lat1_rad) * math.cos(lat2_rad) * math.cos(dlon)\n",
    "    bearing_rad = math.atan2(y, x)\n",
    "    \n",
    "    # Convert bearing from radians to degrees\n",
    "    bearing_deg = math.degrees(bearing_rad)\n",
    "    # Normalize to 0-360 degrees\n",
    "    bearing_deg = (bearing_deg + 360) % 360\n",
    "    \n",
    "    return bearing_deg\n",
    "\n",
    "def create_time_bins(filtered_data, bin_days=7):\n",
    "    \"\"\"\n",
    "    Create time bins and count events in each bin.\n",
    "    \n",
    "    Args:\n",
    "        filtered_data (pandas.DataFrame): Filtered tremor data\n",
    "        bin_days (int): Number of days per bin\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (bin_counts, peak_bins) DataFrames with bin information\n",
    "    \"\"\"\n",
    "    print(f\"Creating {bin_days}-day bins and counting events...\")\n",
    "    min_date = filtered_data['starttime'].min().floor('D')\n",
    "    max_date = filtered_data['starttime'].max().ceil('D')\n",
    "    bin_edges = pd.date_range(start=min_date, end=max_date, freq=f'{bin_days}D')\n",
    "    bin_labels = [bin_edges[i].strftime('%Y-%m-%d') for i in range(len(bin_edges)-1)]\n",
    "    \n",
    "    # Assign each event to a bin\n",
    "    filtered_data['bin'] = pd.cut(filtered_data['starttime'], bins=bin_edges, labels=bin_labels, right=False)\n",
    "    \n",
    "    # Count events in each bin\n",
    "    bin_counts = filtered_data['bin'].value_counts().sort_index()\n",
    "    bin_counts = bin_counts.reset_index()\n",
    "    bin_counts.columns = ['bin_start', 'event_count']\n",
    "    bin_counts['bin_start'] = pd.to_datetime(bin_counts['bin_start'])\n",
    "    bin_counts['bin_end'] = bin_counts['bin_start'] + pd.Timedelta(days=bin_days)\n",
    "    bin_counts['bin_center'] = bin_counts['bin_start'] + pd.Timedelta(days=bin_days/2)\n",
    "    \n",
    "    # Identify peaks with more than threshold events\n",
    "    threshold = 180\n",
    "    peak_bins = bin_counts[bin_counts['event_count'] > threshold]\n",
    "    print(f\"Found {len(peak_bins)} peaks with more than {threshold} events in {bin_days}-day bins\")\n",
    "    \n",
    "    return bin_counts, peak_bins\n",
    "\n",
    "def analyze_peak_migration(filtered_data, peak_bins):\n",
    "    \"\"\"\n",
    "    Analyze migration parameters for each peak.\n",
    "    \n",
    "    Args:\n",
    "        filtered_data (pandas.DataFrame): Filtered tremor data\n",
    "        peak_bins (pandas.DataFrame): Information about peak bins\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (peak_migration_data, peak_summary) with migration analysis\n",
    "    \"\"\"\n",
    "    # Create a list to store migration parameters for each peak\n",
    "    peak_migration_data = []\n",
    "    \n",
    "    # Process each peak\n",
    "    for idx, peak in peak_bins.iterrows():\n",
    "        bin_start = peak['bin_start']\n",
    "        bin_end = peak['bin_end']\n",
    "        bin_label = bin_start.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Extract events for this peak\n",
    "        peak_events = filtered_data[(filtered_data['starttime'] >= bin_start) & \n",
    "                                   (filtered_data['starttime'] < bin_end)]\n",
    "        \n",
    "        # Sort by starttime\n",
    "        peak_events = peak_events.sort_values('starttime')\n",
    "        \n",
    "        # Calculate migration parameters between consecutive events\n",
    "        migration_speeds = []\n",
    "        migration_directions = []\n",
    "        \n",
    "        for i in range(1, len(peak_events)):\n",
    "            prev_event = peak_events.iloc[i-1]\n",
    "            curr_event = peak_events.iloc[i]\n",
    "            \n",
    "            # Calculate distance between events\n",
    "            distance_km = haversine_distance(\n",
    "                prev_event['lat'], prev_event['lon'],\n",
    "                curr_event['lat'], curr_event['lon']\n",
    "            )\n",
    "            \n",
    "            # Calculate time difference in hours\n",
    "            time_diff = (curr_event['starttime'] - prev_event['starttime']).total_seconds() / 3600\n",
    "            \n",
    "            # Calculate migration speed in km/hour\n",
    "            if time_diff > 0:\n",
    "                speed = distance_km / time_diff\n",
    "                migration_speeds.append(speed)\n",
    "            \n",
    "            # Calculate migration direction\n",
    "            direction = calculate_bearing(\n",
    "                prev_event['lat'], prev_event['lon'],\n",
    "                curr_event['lat'], curr_event['lon']\n",
    "            )\n",
    "            migration_directions.append(direction)\n",
    "        \n",
    "        # Calculate average migration speed and direction for this peak\n",
    "        avg_speed = np.mean(migration_speeds) if migration_speeds else np.nan\n",
    "        avg_direction = np.mean(migration_directions) if migration_directions else np.nan\n",
    "        \n",
    "        # Store migration parameters for this peak\n",
    "        peak_migration_data.append({\n",
    "            'bin_start': bin_start,\n",
    "            'bin_end': bin_end,\n",
    "            'bin_label': bin_label,\n",
    "            'event_count': peak['event_count'],\n",
    "            'avg_migration_speed': avg_speed,\n",
    "            'avg_migration_direction': avg_direction,\n",
    "            'migration_speeds': migration_speeds,\n",
    "            'migration_directions': migration_directions,\n",
    "            'peak_events': peak_events\n",
    "        })\n",
    "    \n",
    "    # Create a DataFrame with peak migration data\n",
    "    peak_summary = pd.DataFrame([{\n",
    "        'bin_start': p['bin_start'],\n",
    "        'bin_end': p['bin_end'],\n",
    "        'bin_label': p['bin_label'],\n",
    "        'event_count': p['event_count'],\n",
    "        'avg_migration_speed': p['avg_migration_speed'],\n",
    "        'avg_migration_direction': p['avg_migration_direction']\n",
    "    } for p in peak_migration_data])\n",
    "    \n",
    "    # Save peak summary to CSV\n",
    "    peak_summary.to_csv('peak_migration_summary.csv', index=False)\n",
    "    \n",
    "    return peak_migration_data, peak_summary\n",
    "\n",
    "def plot_event_counts(bin_counts, peak_bins):\n",
    "    \"\"\"\n",
    "    Plot event counts with peaks highlighted.\n",
    "    \n",
    "    Args:\n",
    "        bin_counts (pandas.DataFrame): Counts of events in each bin\n",
    "        peak_bins (pandas.DataFrame): Information about peak bins\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.bar(bin_counts['bin_center'], bin_counts['event_count'], width=6, alpha=0.6, color='gray')\n",
    "    #plt.bar(peak_bins['bin_center'], peak_bins['event_count'], width=6, alpha=0.8, color='red')\n",
    "    \n",
    "    peak_colours = ['green', 'blue', 'orange', 'orange', 'red', 'green','red','green','red','orange','red']  # <- add colors\n",
    "    # make sure the list is as long as peak_bins\n",
    "    if len(peak_colours) < len(peak_bins):\n",
    "        raise ValueError(\"peak_colours must have at least as many entries as peak_bins\")\n",
    "\n",
    "    plt.bar(peak_bins['bin_center'],\n",
    "            peak_bins['event_count'],\n",
    "            width=6,\n",
    "            alpha=0.8,\n",
    "            color=peak_colours[:len(peak_bins)])\n",
    "    \n",
    "    plt.xlabel('Date', fontsize=20)\n",
    "    plt.ylabel('Number of Events', fontsize=20)\n",
    "    plt.title('Tremor Peaks > 180 Events', fontsize=20)\n",
    "    \n",
    "    # Format x-axis to show dates nicely\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.xticks(rotation=45, fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add a horizontal line at 200 events\n",
    "    plt.axhline(y=180, color='black', linestyle='--', alpha=0.7)\n",
    "    plt.text(bin_counts['bin_center'].iloc[0], 210, 'Threshold (180 events)', fontsize=10)\n",
    "    \n",
    "    plt.savefig('plots/event_counts_with_peaks.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_migration_speed(peak_summary):\n",
    "    \"\"\"\n",
    "    Plot migration speed for each peak.\n",
    "    \n",
    "    Args:\n",
    "        peak_summary (pandas.DataFrame): Summary of peak migration parameters\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    bars = plt.bar(peak_summary['bin_label'], peak_summary['avg_migration_speed'], alpha=0.7)\n",
    "    \n",
    "    # Color bars by event count\n",
    "    event_counts = peak_summary['event_count']\n",
    "    norm = plt.Normalize(event_counts.min(), event_counts.max())\n",
    "    colors = plt.cm.viridis(norm(event_counts))\n",
    "    for i, bar in enumerate(bars):\n",
    "        bar.set_color(colors[i])\n",
    "    \n",
    "    plt.xlabel('Peak Period (Start Date)')\n",
    "    plt.ylabel('Average Migration Speed (km/hour)')\n",
    "    plt.title('Migration Speed for Peak Tremor Events (>200 Events in 7-day Bins)')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add a colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=plt.gca())\n",
    "    cbar.set_label('Event Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/peak_migration_speeds.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_migration_direction(peak_summary):\n",
    "    \"\"\"\n",
    "    Plot migration direction for each peak.\n",
    "    \n",
    "    Args:\n",
    "        peak_summary (pandas.DataFrame): Summary of peak migration parameters\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    bars = plt.bar(peak_summary['bin_label'], peak_summary['avg_migration_direction'], alpha=0.7)\n",
    "    \n",
    "    # Color bars by migration speed\n",
    "    speeds = peak_summary['avg_migration_speed']\n",
    "    norm = plt.Normalize(speeds.min(), speeds.max())\n",
    "    colors = plt.cm.plasma(norm(speeds))\n",
    "    for i, bar in enumerate(bars):\n",
    "        bar.set_color(colors[i])\n",
    "    \n",
    "    plt.xlabel('Peak Period (Start Date)')\n",
    "    plt.ylabel('Average Migration Direction (degrees)')\n",
    "    plt.title('Migration Direction for Peak Tremor Events (>200 Events in 7-day Bins)')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add a colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.plasma, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=plt.gca())\n",
    "    cbar.set_label('Migration Speed (km/hour)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/peak_migration_directions.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_speed_vs_direction(peak_summary):\n",
    "    \"\"\"\n",
    "    Create a scatter plot of migration speed vs. direction for each peak.\n",
    "    \n",
    "    Args:\n",
    "        peak_summary (pandas.DataFrame): Summary of peak migration parameters\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(\n",
    "        peak_summary['avg_migration_direction'], \n",
    "        peak_summary['avg_migration_speed'],\n",
    "        c=peak_summary['event_count'],\n",
    "        s=peak_summary['event_count']/5,  # Size points by event count\n",
    "        cmap='viridis',\n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    plt.xlabel('Migration Direction (degrees)')\n",
    "    plt.ylabel('Migration Speed (km/hour)')\n",
    "    plt.title('Migration Speed vs. Direction for Peak Tremor Events')\n",
    "    \n",
    "    # Add a colorbar\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label('Event Count')\n",
    "    \n",
    "    # Add annotations for each point\n",
    "    for i, row in peak_summary.iterrows():\n",
    "        plt.annotate(\n",
    "            row['bin_label'],\n",
    "            (row['avg_migration_direction'], row['avg_migration_speed']),\n",
    "            xytext=(5, 5),\n",
    "            textcoords='offset points',\n",
    "            fontsize=8\n",
    "        )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/speed_vs_direction_scatter.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_polar_directions(peak_summary):\n",
    "    \"\"\"\n",
    "    Create a polar plot of migration directions.\n",
    "    \n",
    "    Args:\n",
    "        peak_summary (pandas.DataFrame): Summary of peak migration parameters\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    \n",
    "    # Convert degrees to radians for polar plot\n",
    "    directions_rad = np.radians(peak_summary['avg_migration_direction'])\n",
    "    speeds = peak_summary['avg_migration_speed']\n",
    "    counts = peak_summary['event_count']\n",
    "    \n",
    "    # Create scatter plot on polar axes\n",
    "    scatter = ax.scatter(\n",
    "        directions_rad, \n",
    "        speeds,\n",
    "        c=counts,\n",
    "        s=counts/5,\n",
    "        cmap='viridis',\n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    # Set the direction labels\n",
    "    ax.set_theta_zero_location('N')\n",
    "    ax.set_theta_direction(-1)  # Clockwise\n",
    "    ax.set_thetagrids([0, 45, 90, 135, 180, 225, 270, 315], \n",
    "                      labels=['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW'])\n",
    "    \n",
    "    # Add a colorbar\n",
    "    cbar = plt.colorbar(scatter, pad=0.1)\n",
    "    cbar.set_label('Event Count')\n",
    "    \n",
    "    # Add annotations for each point\n",
    "    for i, row in peak_summary.iterrows():\n",
    "        direction_rad = np.radians(row['avg_migration_direction'])\n",
    "        speed = row['avg_migration_speed']\n",
    "        ax.annotate(\n",
    "            row['bin_label'],\n",
    "            (direction_rad, speed),\n",
    "            xytext=(5, 5),\n",
    "            textcoords='offset points',\n",
    "            fontsize=8\n",
    "        )\n",
    "    \n",
    "    plt.title('Migration Direction and Speed (Polar Plot)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/polar_migration_directions.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def create_density_map(peak_events,resolution=50):#,bin_label\n",
    "    \"\"\"\n",
    "    Create a 2D histogram (density map) for a peak cluster.\n",
    "    \n",
    "    Args:\n",
    "        peak_events (pandas.DataFrame): Events for a specific peak\n",
    "        resolution (int): Resolution of the density map\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (hist_normalized, lon_edges, lat_edges) for the density map\n",
    "    \"\"\"\n",
    "    lon_min, lon_max = peak_events['lon'].min() - 0.01, peak_events['lon'].max() + 0.01\n",
    "    lat_min, lat_max = peak_events['lat'].min() - 0.01, peak_events['lat'].max() + 0.01\n",
    "    \n",
    "    # Create bins for the 2D histogram\n",
    "    lon_bins = np.linspace(lon_min, lon_max, resolution)\n",
    "    lat_bins = np.linspace(lat_min, lat_max, resolution)\n",
    "    \n",
    "    # Create the 2D histogram\n",
    "    hist, lon_edges, lat_edges = np.histogram2d(\n",
    "        peak_events['lon'], peak_events['lat'], \n",
    "        bins=[lon_bins, lat_bins]\n",
    "    )\n",
    "    \n",
    "    # Normalize the histogram\n",
    "    hist_normalized = hist / hist.max()\n",
    "    \n",
    "    #print(f\"Creating density map for {bin_label}: {np.sum(hist_normalized):.4f} total density\")\n",
    "    \n",
    "    return hist_normalized, lon_edges, lat_edges\n",
    "\n",
    "def extract_features_from_density_map(density_map):\n",
    "    \"\"\"\n",
    "    Extract features from a density map for clustering.\n",
    "    \n",
    "    Args:\n",
    "        density_map (numpy.ndarray): 2D density map\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Feature vector\n",
    "    \"\"\"\n",
    "    # Calculate basic statistics\n",
    "    mean_density = np.mean(density_map)\n",
    "    std_density = np.std(density_map)\n",
    "    max_density = np.max(density_map)\n",
    "    \n",
    "    # Calculate the center of mass\n",
    "    y_indices, x_indices = np.indices(density_map.shape)\n",
    "    center_y = np.sum(y_indices * density_map) / np.sum(density_map)\n",
    "    center_x = np.sum(x_indices * density_map) / np.sum(density_map)\n",
    "    \n",
    "    # Calculate spatial moments\n",
    "    y_diff = y_indices - center_y\n",
    "    x_diff = x_indices - center_x\n",
    "    moment_y2 = np.sum(y_diff**2 * density_map) / np.sum(density_map)\n",
    "    moment_x2 = np.sum(x_diff**2 * density_map) / np.sum(density_map)\n",
    "    moment_xy = np.sum(y_diff * x_diff * density_map) / np.sum(density_map)\n",
    "    \n",
    "    # Calculate the number of hotspots (local maxima)\n",
    "    from scipy import ndimage\n",
    "    from skimage import feature\n",
    "    local_max = feature.peak_local_max(density_map, min_distance=3)\n",
    "    num_hotspots = len(local_max)\n",
    "    \n",
    "    # Calculate the area above certain thresholds\n",
    "    area_10 = np.sum(density_map > 0.1)\n",
    "    area_25 = np.sum(density_map > 0.25)\n",
    "    area_50 = np.sum(density_map > 0.5)\n",
    "    \n",
    "    # Create a feature vector\n",
    "    features = np.array([\n",
    "        mean_density, std_density, max_density,\n",
    "        center_y, center_x,\n",
    "        moment_y2, moment_x2, moment_xy,\n",
    "        num_hotspots,\n",
    "        area_10, area_25, area_50\n",
    "    ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def create_map_visualizations(filtered_data, peak_summary):\n",
    "    \"\"\"\n",
    "    Create map visualizations for each peak cluster.\n",
    "    \n",
    "    Args:\n",
    "        filtered_data (pandas.DataFrame): Filtered tremor data\n",
    "        peak_summary (pandas.DataFrame): Summary of peak migration parameters\n",
    "    \"\"\"\n",
    "    # Create a custom colormap for time progression\n",
    "    colors = [(0, 0, 0.8), (0, 0.8, 0), (0.8, 0.8, 0), (0.8, 0, 0)]  # Blue -> Green -> Yellow -> Red\n",
    "    cmap_name = 'time_progression'\n",
    "    cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=100)\n",
    "    \n",
    "    print(f\"Creating map visualizations for {len(peak_summary)} peak clusters...\")\n",
    "    \n",
    "    # Process each peak\n",
    "    for i, peak in peak_summary.iterrows():\n",
    "        bin_start = peak['bin_start']\n",
    "        bin_end = peak['bin_end']\n",
    "        bin_label = peak['bin_label']\n",
    "        \n",
    "        print(f\"Processing peak {i+1}/{len(peak_summary)}: {bin_label}\")\n",
    "        \n",
    "        # Extract events for this peak\n",
    "        peak_events = filtered_data[(filtered_data['starttime'] >= bin_start) & \n",
    "                                   (filtered_data['starttime'] < bin_end)]\n",
    "        \n",
    "        # Sort by starttime\n",
    "        peak_events = peak_events.sort_values('starttime')\n",
    "        \n",
    "        # Create a figure with two subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        \n",
    "        # First plot: Scatter plot with basemap\n",
    "        scatter = ax1.scatter(\n",
    "            peak_events['lon'], \n",
    "            peak_events['lat'],\n",
    "            c=range(len(peak_events)),  # Color by event order\n",
    "            cmap=cm,\n",
    "            alpha=0.7,\n",
    "            s=50,\n",
    "            zorder=3\n",
    "        )\n",
    "        \n",
    "\n",
    "        ax1.set_facecolor('#f2f2f2')\n",
    "        \n",
    "        # Connect events with lines to show migration path\n",
    "        ax1.plot(peak_events['lon'], peak_events['lat'], 'k-', alpha=0.3, zorder=2)\n",
    "        \n",
    "        # Add first and last event markers\n",
    "        ax1.plot(peak_events['lon'].iloc[0], peak_events['lat'].iloc[0], 'o', \n",
    "                 color='blue', markersize=12, markeredgecolor='black', zorder=4)\n",
    "        ax1.plot(peak_events['lon'].iloc[-1], peak_events['lat'].iloc[-1], 'o', \n",
    "                 color='red', markersize=12, markeredgecolor='black', zorder=4)\n",
    "        \n",
    "        # Add text labels with white outline for visibility\n",
    "        start_txt = ax1.text(peak_events['lon'].iloc[0] + 0.01, peak_events['lat'].iloc[0] + 0.01, \n",
    "                             'Start', fontsize=12, color='white', fontweight='bold', zorder=5)\n",
    "        start_txt.set_path_effects([PathEffects.withStroke(linewidth=3, foreground='black')])\n",
    "        \n",
    "        end_txt = ax1.text(peak_events['lon'].iloc[-1] + 0.01, peak_events['lat'].iloc[-1] + 0.01, \n",
    "                           'End', fontsize=12, color='white', fontweight='bold', zorder=5)\n",
    "        end_txt.set_path_effects([PathEffects.withStroke(linewidth=3, foreground='black')])\n",
    "        \n",
    "        ax1.set_xlabel('Longitude')\n",
    "        ax1.set_ylabel('Latitude')\n",
    "        ax1.set_title(f'Tremor Events Map: {bin_label} to {bin_end.strftime(\"%Y-%m-%d\")}')\n",
    "        \n",
    "        # Add colorbar for time progression\n",
    "        cbar = fig.colorbar(scatter, ax=ax1)\n",
    "        cbar.set_label('Event Sequence (Time Progression)')\n",
    "        \n",
    "        # Add text with migration parameters\n",
    "        textstr = (f'Event Count: {peak[\"event_count\"]}\\n'\n",
    "                   f'Avg Migration Speed: {peak[\"avg_migration_speed\"]:.2f} km/h\\n'\n",
    "                   f'Avg Migration Direction: {peak[\"avg_migration_direction\"]:.2f}°')\n",
    "        \n",
    "        props = dict(boxstyle='round', facecolor='white', alpha=0.7)\n",
    "        ax1.text(0.05, 0.05, textstr, transform=ax1.transAxes, fontsize=12,\n",
    "                 verticalalignment='bottom', bbox=props)\n",
    "        \n",
    "        # Second plot: Heatmap with basemap\n",
    "        # Create a 2D histogram\n",
    "        lon_bins = np.linspace(peak_events['lon'].min() - 0.01, peak_events['lon'].max() + 0.01, 50)\n",
    "        lat_bins = np.linspace(peak_events['lat'].min() - 0.01, peak_events['lat'].max() + 0.01, 50)\n",
    "        \n",
    "        hist, lon_edges, lat_edges = np.histogram2d(\n",
    "            peak_events['lon'], peak_events['lat'], \n",
    "            bins=[lon_bins, lat_bins]\n",
    "        )\n",
    "        \n",
    "        # Plot the heatmap\n",
    "        lon_centers = (lon_edges[:-1] + lon_edges[1:]) / 2\n",
    "        lat_centers = (lat_edges[:-1] + lat_edges[1:]) / 2\n",
    "        lon_mesh, lat_mesh = np.meshgrid(lon_centers, lat_centers)\n",
    "        \n",
    "        heatmap = ax2.pcolormesh(lon_mesh, lat_mesh, hist.T, cmap='hot', alpha=0.7, zorder=3)\n",
    "        \n",
    "        ax2.set_facecolor('#f2f2f2')\n",
    "        \n",
    "        ax2.set_xlabel('Longitude')\n",
    "        ax2.set_ylabel('Latitude')\n",
    "        ax2.set_title(f'Tremor Event Density: {bin_label} to {bin_end.strftime(\"%Y-%m-%d\")}')\n",
    "        \n",
    "        # Add colorbar for heatmap\n",
    "        cbar2 = fig.colorbar(heatmap, ax=ax2)\n",
    "        cbar2.set_label('Event Density')\n",
    "        \n",
    "        # Ensure both plots have the same extent\n",
    "        xlim = [min(peak_events['lon'].min() - 0.02, lon_edges.min()), \n",
    "                max(peak_events['lon'].max() + 0.02, lon_edges.max())]\n",
    "        ylim = [min(peak_events['lat'].min() - 0.02, lat_edges.min()), \n",
    "                max(peak_events['lat'].max() + 0.02, lat_edges.max())]\n",
    "        \n",
    "        ax1.set_xlim(xlim)\n",
    "        ax1.set_ylim(ylim)\n",
    "        ax2.set_xlim(xlim)\n",
    "        ax2.set_ylim(ylim)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'maps/map_cluster_{bin_label}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    print(\"Map visualizations complete. Results saved to maps directory.\")\n",
    "\n",
    "def perform_hierarchical_clustering(peak_summary, filtered_data, peak_migration_data):\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering on peak clusters.\n",
    "    \n",
    "    Args:\n",
    "        peak_summary (pandas.DataFrame): Summary of peak migration parameters\n",
    "        filtered_data (pandas.DataFrame): Filtered tremor data\n",
    "        peak_migration_data (list): Detailed migration data for each peak\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Clustering results\n",
    "    \"\"\"\n",
    "    print(\"Performing hierarchical clustering on peak clusters...\")\n",
    "    \n",
    "    # Process each peak to create density maps and extract features\n",
    "    density_maps = []\n",
    "    density_map_metadata = []  # Store lon/lat edges\n",
    "    feature_vectors = []\n",
    "    peak_labels = []\n",
    "    \n",
    "    for i, peak in peak_summary.iterrows():\n",
    "        bin_start = peak['bin_start']\n",
    "        bin_end = peak['bin_end']\n",
    "        bin_label = peak['bin_label']\n",
    "        \n",
    "        print(f\"Processing peak {i+1}/{len(peak_summary)}: {bin_label}\")\n",
    "        \n",
    "        # Extract events for this peak\n",
    "        peak_events = filtered_data[(filtered_data['starttime'] >= bin_start) & \n",
    "                                   (filtered_data['starttime'] < bin_end)]\n",
    "        \n",
    "        # Get spatial bounds\n",
    "        lon_min, lon_max = peak_events['lon'].min(), peak_events['lon'].max()\n",
    "        lat_min, lat_max = peak_events['lat'].min(), peak_events['lat'].max()\n",
    "\n",
    "        print(f\"Peak {bin_label}: Lon ({lon_min:.4f}, {lon_max:.4f}) | Lat ({lat_min:.4f}, {lat_max:.4f})\")\n",
    "        \n",
    "        # Create density map\n",
    "        density_map, lon_edges, lat_edges = create_density_map(peak_events)#, bin_label)\n",
    "        density_maps.append(density_map)\n",
    "        density_map_metadata.append((lon_edges, lat_edges))  # Store bin edges\n",
    "        \n",
    "        # Extract features\n",
    "        features = extract_features_from_density_map(density_map)\n",
    "        \n",
    "        # Store results\n",
    "        density_maps.append(density_map)\n",
    "        feature_vectors.append(features)\n",
    "        peak_labels.append(bin_label)\n",
    "    \n",
    "    # Convert feature vectors to a numpy array\n",
    "    feature_array = np.array(feature_vectors)\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(feature_array)\n",
    "    \n",
    "    # Apply PCA for dimensionality reduction and visualization\n",
    "    pca = PCA(n_components=min(len(peak_summary) - 1, 5))\n",
    "    pca_result = pca.fit_transform(scaled_features)\n",
    "    \n",
    "    print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    print(f\"Total explained variance: {sum(pca.explained_variance_ratio_):.2f}\")\n",
    "    \n",
    "    # Create a figure to visualize PCA results\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], s=100)\n",
    "    \n",
    "    # Add labels for each point\n",
    "    for i, label in enumerate(peak_labels):\n",
    "        plt.annotate(label, (pca_result[i, 0], pca_result[i, 1]), \n",
    "                     fontsize=9, ha='center')\n",
    "    \n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2f})')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2f})')\n",
    "    plt.title('PCA of Density Map Features')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('clustering_results/pca_visualization.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Perform hierarchical clustering\n",
    "    # Calculate distance matrix\n",
    "    dist_matrix = pdist(scaled_features, metric='euclidean')\n",
    "    \n",
    "    # Perform hierarchical clustering with different linkage methods\n",
    "    linkage_methods = ['single', 'complete', 'average', 'ward']\n",
    "    linkage_results = {}\n",
    "    \n",
    "    for method in linkage_methods:\n",
    "        linkage_results[method] = linkage(scaled_features, method=method)\n",
    "    \n",
    "    # Create dendrograms for each linkage method\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    for i, method in enumerate(linkage_methods):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        dendrogram(linkage_results[method], labels=peak_labels, orientation='top', leaf_font_size=9)\n",
    "        plt.title(f'Hierarchical Clustering Dendrogram ({method} linkage)')\n",
    "        plt.xlabel('Peak Clusters')\n",
    "        plt.ylabel('Distance')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('clustering_results/dendrograms_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Determine optimal number of clusters using silhouette score\n",
    "    silhouette_scores = []\n",
    "    K_range = range(2, min(6, len(peak_summary)))\n",
    "    \n",
    "    # Use ward linkage for final clustering\n",
    "    Z = linkage_results['ward']\n",
    "    \n",
    "    for k in K_range:\n",
    "        # Get cluster labels\n",
    "        cluster_labels = fcluster(Z, k, criterion='maxclust')\n",
    "        \n",
    "        if len(set(cluster_labels)) > 1:  # Ensure we have at least 2 clusters\n",
    "            score = silhouette_score(scaled_features, cluster_labels)\n",
    "            silhouette_scores.append(score)\n",
    "            print(f\"Silhouette score for k={k}: {score:.3f}\")\n",
    "        else:\n",
    "            silhouette_scores.append(0)\n",
    "            print(f\"Only one cluster found for k={k}\")\n",
    "    \n",
    "    # Plot silhouette scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(list(K_range), silhouette_scores, 'o-')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Score for Different Numbers of Clusters')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('clustering_results/silhouette_scores.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Choose optimal k based on silhouette score\n",
    "    if silhouette_scores:\n",
    "        optimal_k = list(K_range)[np.argmax(silhouette_scores)]\n",
    "    else:\n",
    "        optimal_k = 2  # Default if silhouette scores couldn't be calculated\n",
    "    \n",
    "    print(f\"Optimal number of clusters based on silhouette score: {optimal_k}\")\n",
    "    \n",
    "    # Get final cluster labels\n",
    "    hierarchical_labels = fcluster(Z, optimal_k, criterion='maxclust')\n",
    "    \n",
    "    # Create a DataFrame to store clustering results\n",
    "    clustering_results = pd.DataFrame({\n",
    "        'peak_label': peak_labels,\n",
    "        'hierarchical_cluster': hierarchical_labels,\n",
    "        'avg_migration_speed': peak_summary['avg_migration_speed'].values,\n",
    "        'avg_migration_direction': peak_summary['avg_migration_direction'].values,\n",
    "        'event_count': peak_summary['event_count'].values\n",
    "    })\n",
    "    \n",
    "    # Save clustering results\n",
    "    clustering_results.to_csv('clustering_results/hierarchical_clustering_results.csv', index=False)\n",
    "    \n",
    "    print(\"Clustering results:\")\n",
    "    print(clustering_results)\n",
    "    \n",
    "    # Create a detailed dendrogram for the final clustering\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    dendrogram(\n",
    "        Z,\n",
    "        labels=peak_labels,\n",
    "        orientation='top',\n",
    "        leaf_font_size=9,\n",
    "        color_threshold=Z[-(optimal_k-1), 2]  # Color threshold to show optimal_k clusters\n",
    "    )\n",
    "    plt.title('Hierarchical Clustering Dendrogram (Ward Linkage)')\n",
    "    plt.xlabel('Peak Clusters', fontsize=16)\n",
    "    plt.ylabel('Distance', fontsize=16)\n",
    "    plt.xticks(rotation=45, ha='right',fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    x_labels = plt.gca().get_xticklabels()\n",
    "\n",
    "    # Make the 6th and 7th labels bold (indices 5 and 6 are the 6th and 7th, 0-based)\n",
    "    x_labels[5].set_fontweight('bold')\n",
    "    x_labels[6].set_fontweight('bold')\n",
    "\n",
    "    # Assign them back to the axis\n",
    "    plt.gca().set_xticklabels(x_labels)\n",
    "    \n",
    "    # Add a horizontal line at the cut threshold\n",
    "    plt.axhline(y=Z[-(optimal_k-1), 2], color='r', linestyle='--')\n",
    "    plt.text(0, Z[-(optimal_k-1), 2] * 1.05, f'Cut threshold: {Z[-(optimal_k-1), 2]:.2f}', color='r')\n",
    "    \n",
    "    plt.savefig('clustering_results/final_dendrogram.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Visualize clustering results in PCA space\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(\n",
    "        pca_result[:, 0], \n",
    "        pca_result[:, 1], \n",
    "        c=hierarchical_labels, \n",
    "        cmap='viridis', \n",
    "        s=100, \n",
    "        alpha=0.8\n",
    "    )\n",
    "    \n",
    "    # Add labels for each point\n",
    "    for i, label in enumerate(peak_labels):\n",
    "        plt.annotate(\n",
    "            label, \n",
    "            (pca_result[i, 0], pca_result[i, 1]), \n",
    "            fontsize=9, \n",
    "            ha='center', \n",
    "            va='bottom',\n",
    "            xytext=(0, 5), \n",
    "            textcoords='offset points'\n",
    "        )\n",
    "    \n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2f})')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2f})')\n",
    "    plt.title('Hierarchical Clustering Results')\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('clustering_results/hierarchical_clustering_pca.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Analyze cluster characteristics\n",
    "    cluster_stats = clustering_results.groupby('hierarchical_cluster').agg({\n",
    "        'peak_label': ['count', list],\n",
    "        'avg_migration_speed': ['mean', 'std', 'min', 'max'],\n",
    "        'avg_migration_direction': ['mean', 'std', 'min', 'max'],\n",
    "        'event_count': ['mean', 'std', 'min', 'max']\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten the multi-level column names\n",
    "    cluster_stats.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col for col in cluster_stats.columns]\n",
    "    \n",
    "    # Save cluster statistics\n",
    "    cluster_stats.to_csv('clustering_results/hierarchical_cluster_statistics.csv', index=False)\n",
    "    \n",
    "    print(\"Cluster statistics:\")\n",
    "    for i, row in cluster_stats.iterrows():\n",
    "        print(f\"Cluster {row['hierarchical_cluster']}: {row['peak_label_count']} peaks\")\n",
    "        print(f\"  Peaks: {', '.join(row['peak_label_list'])}\")\n",
    "        print(f\"  Avg migration speed: {row['avg_migration_speed_mean']:.2f} km/h (±{row['avg_migration_speed_std']:.2f})\")\n",
    "        print(f\"  Avg migration direction: {row['avg_migration_direction_mean']:.2f}° (±{row['avg_migration_direction_std']:.2f})\")\n",
    "        print(f\"  Avg event count: {row['event_count_mean']:.1f} (±{row['event_count_std']:.1f})\")\n",
    "        print()\n",
    "    ###########################################################################################\n",
    "    # Visualize representative density maps for each cluster\n",
    "    for cluster_id in range(1, optimal_k + 1):  # Cluster IDs start from 1\n",
    "        # Get indices of peaks in this cluster\n",
    "        cluster_indices = np.where(hierarchical_labels == cluster_id)[0]\n",
    "\n",
    "        if len(cluster_indices) == 0:\n",
    "            continue\n",
    "\n",
    "        # Create a figure for this cluster\n",
    "        fig_width = min(20, 5 * len(cluster_indices))  # Limit width for many maps\n",
    "        fig, axes = plt.subplots(1, len(cluster_indices), figsize=(fig_width, 5))\n",
    "\n",
    "        # Handle the case where there's only one map in the cluster\n",
    "        if len(cluster_indices) == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        # Plot each density map in this cluster\n",
    "        for i, idx in enumerate(cluster_indices):\n",
    "            axes[i].imshow(density_maps[idx], cmap='hot', origin='lower')\n",
    "            axes[i].set_title(f\"{peak_labels[idx]}\\nCluster {cluster_id}\")\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'clustering_results/cluster_{cluster_id}_density_maps.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    ##############################################################################################\n",
    "    # Visualize representative density maps for each cluster with larger y-axis scaling\n",
    "#     for cluster_id in range(1, optimal_k + 1):  # Cluster IDs start from 1\n",
    "#         # Get indices of peaks in this cluster\n",
    "#         cluster_indices = np.where(hierarchical_labels == cluster_id)[0]\n",
    "\n",
    "#         if len(cluster_indices) == 0:\n",
    "#             continue\n",
    "\n",
    "#         # Set figure width dynamically, but increase height\n",
    "#         fig_width = min(20, 5 * len(cluster_indices))  # Limit width for many maps\n",
    "#         fig_height = 4  # Increase height for better y-axis visualization\n",
    "#         fig, axes = plt.subplots(1, len(cluster_indices), figsize=(fig_width, fig_height))\n",
    "\n",
    "#         # Handle case where there's only one map in the cluster\n",
    "#         if len(cluster_indices) == 1:\n",
    "#             axes = [axes]\n",
    "\n",
    "#         # Plot each density map in this cluster\n",
    "#         for i, idx in enumerate(cluster_indices):\n",
    "#             density_map = density_maps[idx]\n",
    "#             lon_edges, lat_edges = density_map_metadata[idx]  # Get stored bin edges\n",
    "\n",
    "#             # Compute bin centers for plotting\n",
    "#             lon_centers = (lon_edges[:-1] + lon_edges[1:]) / 2\n",
    "#             lat_centers = (lat_edges[:-1] + lat_edges[1:]) / 2\n",
    "\n",
    "#             # Plot heatmap with latitude and longitude\n",
    "#             im = axes[i].imshow(density_map, cmap='hot', origin='lower',\n",
    "#                                 extent=[lon_edges[0], lon_edges[-1], lat_edges[0], lat_edges[-1]],\n",
    "#                                 aspect='auto')  # Allow the aspect ratio to adjust dynamically\n",
    "\n",
    "#             axes[i].set_title(f\"{peak_labels[idx]}\\nCluster {cluster_id}\", fontsize=12)\n",
    "#             axes[i].set_xlabel('Longitude', fontsize=10)\n",
    "#             axes[i].set_ylabel('Latitude', fontsize=10)\n",
    "\n",
    "#             # Add colorbar for density\n",
    "#             cbar = plt.colorbar(im, ax=axes[i], shrink=0.7)\n",
    "#             cbar.set_label('Event Density', fontsize=10)\n",
    "\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f'clustering_results/cluster_{cluster_id}_density_maps.png', dpi=300, bbox_inches='tight')\n",
    "#         plt.close()\n",
    "\n",
    "  \n",
    "    \n",
    "    \n",
    "    # Create a visualization comparing migration parameters between clusters\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Migration Speed by Cluster\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for cluster_id in range(1, optimal_k + 1):\n",
    "        cluster_data = clustering_results[clustering_results['hierarchical_cluster'] == cluster_id]\n",
    "        plt.scatter(\n",
    "            cluster_data['peak_label'], \n",
    "            cluster_data['avg_migration_speed'],\n",
    "            label=f'Cluster {cluster_id}',\n",
    "            s=80,\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    plt.xlabel('Peak Period')\n",
    "    plt.ylabel('Migration Speed (km/h)')\n",
    "    plt.title('Migration Speed by Cluster')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Migration Direction by Cluster\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for cluster_id in range(1, optimal_k + 1):\n",
    "        cluster_data = clustering_results[clustering_results['hierarchical_cluster'] == cluster_id]\n",
    "        plt.scatter(\n",
    "            cluster_data['peak_label'], \n",
    "            cluster_data['avg_migration_direction'],\n",
    "            label=f'Cluster {cluster_id}',\n",
    "            s=80,\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    plt.xlabel('Peak Period')\n",
    "    plt.ylabel('Migration Direction (degrees)')\n",
    "    plt.title('Migration Direction by Cluster')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Event Count by Cluster\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for cluster_id in range(1, optimal_k + 1):\n",
    "        cluster_data = clustering_results[clustering_results['hierarchical_cluster'] == cluster_id]\n",
    "        plt.scatter(\n",
    "            cluster_data['peak_label'], \n",
    "            cluster_data['event_count'],\n",
    "            label=f'Cluster {cluster_id}',\n",
    "            s=80,\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    plt.xlabel('Peak Period')\n",
    "    plt.ylabel('Event Count')\n",
    "    plt.title('Event Count by Cluster')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Migration Speed vs Direction by Cluster\n",
    "    plt.subplot(2, 2, 4)\n",
    "    for cluster_id in range(1, optimal_k + 1):\n",
    "        cluster_data = clustering_results[clustering_results['hierarchical_cluster'] == cluster_id]\n",
    "        plt.scatter(\n",
    "            cluster_data['avg_migration_direction'], \n",
    "            cluster_data['avg_migration_speed'],\n",
    "            label=f'Cluster {cluster_id}',\n",
    "            s=cluster_data['event_count'] / 5,  # Size by event count\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    plt.xlabel('Migration Direction (degrees)')\n",
    "    plt.ylabel('Migration Speed (km/h)')\n",
    "    plt.title('Migration Speed vs Direction by Cluster')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('clustering_results/cluster_parameter_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a polar plot showing migration parameters by cluster\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    \n",
    "    # Plot each cluster with a different color\n",
    "    for cluster_id in range(1, optimal_k + 1):\n",
    "        cluster_data = clustering_results[clustering_results['hierarchical_cluster'] == cluster_id]\n",
    "        \n",
    "        # Convert degrees to radians for polar plot\n",
    "        directions_rad = np.radians(cluster_data['avg_migration_direction'])\n",
    "        speeds = cluster_data['avg_migration_speed']\n",
    "        counts = cluster_data['event_count']\n",
    "        \n",
    "        ax.scatter(\n",
    "            directions_rad, \n",
    "            speeds,\n",
    "            label=f'Cluster {cluster_id}',\n",
    "            s=counts / 5,  # Size by event count\n",
    "            alpha=0.7\n",
    "        )\n",
    "        \n",
    "        # Add labels for each point\n",
    "        for i, row in cluster_data.iterrows():\n",
    "            direction_rad = np.radians(row['avg_migration_direction'])\n",
    "            ax.annotate(\n",
    "                row['peak_label'],\n",
    "                (direction_rad, row['avg_migration_speed']),\n",
    "                xytext=(5, 5),\n",
    "                textcoords='offset points',\n",
    "                fontsize=8\n",
    "            )\n",
    "    \n",
    "    # Set the direction labels\n",
    "    ax.set_theta_zero_location('N')\n",
    "    ax.set_theta_direction(-1)  # Clockwise\n",
    "    ax.set_thetagrids([0, 45, 90, 135, 180, 225, 270, 315], \n",
    "                      labels=['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW'])\n",
    "    \n",
    "    plt.title('Migration Parameters by Cluster (Polar Plot)')\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('clustering_results/polar_cluster_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Hierarchical clustering analysis complete. Results saved to clustering_results directory.\")\n",
    "    \n",
    "    return clustering_results, density_maps, peak_labels, hierarchical_labels, optimal_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12cb1582",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tremor events data from /home/seismic/tremor_events-2009-08-10T00_00_00-2025-03-17T23_59_59.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3428557/375983614.py:12: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tremor_data = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 7-day bins and counting events...\n",
      "Found 11 peaks with more than 180 events in 7-day bins\n",
      "Creating map visualizations for 11 peak clusters...\n",
      "Processing peak 1/11: 2009-09-02\n",
      "Processing peak 2/11: 2011-06-08\n",
      "Processing peak 3/11: 2013-03-06\n",
      "Processing peak 4/11: 2016-02-03\n",
      "Processing peak 5/11: 2017-12-06\n",
      "Processing peak 6/11: 2019-05-15\n",
      "Processing peak 7/11: 2021-04-14\n",
      "Processing peak 8/11: 2022-10-12\n",
      "Processing peak 9/11: 2024-04-17\n",
      "Processing peak 10/11: 2024-04-24\n",
      "Processing peak 11/11: 2024-10-02\n",
      "Map visualizations complete. Results saved to maps directory.\n",
      "Performing hierarchical clustering on peak clusters...\n",
      "Processing peak 1/11: 2009-09-02\n",
      "Peak 2009-09-02: Lon (-123.9400, -122.6300) | Lat (44.4000, 44.6000)\n",
      "Processing peak 2/11: 2011-06-08\n",
      "Peak 2011-06-08: Lon (-123.7800, -123.2250) | Lat (44.4000, 44.6000)\n",
      "Processing peak 3/11: 2013-03-06\n",
      "Peak 2013-03-06: Lon (-123.8350, -123.1800) | Lat (44.4000, 44.6000)\n",
      "Processing peak 4/11: 2016-02-03\n",
      "Peak 2016-02-03: Lon (-123.9200, -123.1900) | Lat (44.4000, 44.6000)\n",
      "Processing peak 5/11: 2017-12-06\n",
      "Peak 2017-12-06: Lon (-124.0262, -122.8874) | Lat (44.4007, 44.5991)\n",
      "Processing peak 6/11: 2019-05-15\n",
      "Peak 2019-05-15: Lon (-123.9882, -123.0381) | Lat (44.4001, 44.5998)\n",
      "Processing peak 7/11: 2021-04-14\n",
      "Peak 2021-04-14: Lon (-123.9495, -122.9524) | Lat (44.4009, 44.5999)\n",
      "Processing peak 8/11: 2022-10-12\n",
      "Peak 2022-10-12: Lon (-124.2915, -123.0141) | Lat (44.4015, 44.5998)\n",
      "Processing peak 9/11: 2024-04-17\n",
      "Peak 2024-04-17: Lon (-123.9908, -122.9917) | Lat (44.4003, 44.5999)\n",
      "Processing peak 10/11: 2024-04-24\n",
      "Peak 2024-04-24: Lon (-124.1006, -122.8864) | Lat (44.4003, 44.5998)\n",
      "Processing peak 11/11: 2024-10-02\n",
      "Peak 2024-10-02: Lon (-124.0312, -123.2940) | Lat (44.4007, 44.5968)\n",
      "PCA explained variance ratio: [0.35805548 0.25206856 0.14704638 0.12770463 0.06796624]\n",
      "Total explained variance: 0.95\n",
      "Silhouette score for k=2: 0.194\n",
      "Silhouette score for k=3: 0.199\n",
      "Silhouette score for k=4: 0.223\n",
      "Silhouette score for k=5: 0.180\n",
      "Optimal number of clusters based on silhouette score: 4\n",
      "Clustering results:\n",
      "    peak_label  hierarchical_cluster  avg_migration_speed  \\\n",
      "0   2009-09-02                     2           167.118090   \n",
      "1   2011-06-08                     4           127.458357   \n",
      "2   2013-03-06                     1           147.440153   \n",
      "3   2016-02-03                     1           124.677649   \n",
      "4   2017-12-06                     3           204.819408   \n",
      "5   2019-05-15                     2           183.177632   \n",
      "6   2021-04-14                     3           168.569967   \n",
      "7   2022-10-12                     2           194.483183   \n",
      "8   2024-04-17                     1           144.246659   \n",
      "9   2024-04-24                     3           189.000058   \n",
      "10  2024-10-02                     1           163.948227   \n",
      "\n",
      "    avg_migration_direction  event_count  \n",
      "0                178.004754          389  \n",
      "1                180.513057          250  \n",
      "2                187.985717          190  \n",
      "3                192.941636          207  \n",
      "4                171.810066          405  \n",
      "5                174.124264          409  \n",
      "6                187.101029          318  \n",
      "7                179.373843          446  \n",
      "8                171.928706          211  \n",
      "9                174.327155          245  \n",
      "10               178.798008          236  \n",
      "Cluster statistics:\n",
      "Cluster 1: 4 peaks\n",
      "  Peaks: 2013-03-06, 2016-02-03, 2024-04-17, 2024-10-02\n",
      "  Avg migration speed: 145.08 km/h (±16.11)\n",
      "  Avg migration direction: 182.91° (±9.38)\n",
      "  Avg event count: 211.0 (±19.0)\n",
      "\n",
      "Cluster 2: 3 peaks\n",
      "  Peaks: 2009-09-02, 2019-05-15, 2022-10-12\n",
      "  Avg migration speed: 181.59 km/h (±13.75)\n",
      "  Avg migration direction: 177.17° (±2.72)\n",
      "  Avg event count: 414.7 (±28.9)\n",
      "\n",
      "Cluster 3: 3 peaks\n",
      "  Peaks: 2017-12-06, 2021-04-14, 2024-04-24\n",
      "  Avg migration speed: 187.46 km/h (±18.17)\n",
      "  Avg migration direction: 177.75° (±8.20)\n",
      "  Avg event count: 322.7 (±80.1)\n",
      "\n",
      "Cluster 4: 1 peaks\n",
      "  Peaks: 2011-06-08\n",
      "  Avg migration speed: 127.46 km/h (±nan)\n",
      "  Avg migration direction: 180.51° (±nan)\n",
      "  Avg event count: 250.0 (±nan)\n",
      "\n",
      "Hierarchical clustering analysis complete. Results saved to clustering_results directory.\n",
      "Hierarchical clustering analysis complete!\n",
      "- 4329 tremor events analyzed\n",
      "- 11 peak periods identified\n",
      "- 4 hierarchical clusters created\n",
      "- Results saved to plots/, maps/, and clustering_results/ directories\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Main function to run the hierarchical clustering analysis.\"\"\"\n",
    "# Check if a file path was provided as a command-line argument\n",
    "file_path = '/home/seismic/tremor_events-2009-08-10T00_00_00-2025-03-17T23_59_59.csv'\n",
    "\n",
    "# Load and preprocess data\n",
    "tremor_data = load_data(file_path)\n",
    "filtered_data = filter_data(tremor_data)\n",
    "\n",
    "# Create time bins and analyze peaks\n",
    "bin_counts, peak_bins = create_time_bins(filtered_data)\n",
    "peak_migration_data, peak_summary = analyze_peak_migration(filtered_data, peak_bins)\n",
    "\n",
    "# Create basic visualizations\n",
    "plot_event_counts(bin_counts, peak_bins)\n",
    "plot_migration_speed(peak_summary)\n",
    "plot_migration_direction(peak_summary)\n",
    "plot_speed_vs_direction(peak_summary)\n",
    "plot_polar_directions(peak_summary)\n",
    "\n",
    "# Create map visualizations\n",
    "create_map_visualizations(filtered_data, peak_summary)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "clustering_results, density_maps, peak_labels, hierarchical_labels, optimal_k = perform_hierarchical_clustering(\n",
    "    peak_summary, filtered_data, peak_migration_data\n",
    ")\n",
    "\n",
    "print(\"Hierarchical clustering analysis complete!\")\n",
    "print(f\"- {len(filtered_data)} tremor events analyzed\")\n",
    "print(f\"- {len(peak_summary)} peak periods identified\")\n",
    "print(f\"- {optimal_k} hierarchical clusters created\")\n",
    "print(f\"- Results saved to plots/, maps/, and clustering_results/ directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e67f6284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for enhanced visualizations\n",
    "os.makedirs('enhanced_visualizations', exist_ok=True)\n",
    "\n",
    "def load_clustering_results(file_path='clustering_results/hierarchical_clustering_results.csv'):\n",
    "    \"\"\"\n",
    "    Load hierarchical clustering results from CSV file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the clustering results CSV file\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Loaded clustering results\n",
    "    \"\"\"\n",
    "    print(f\"Loading clustering results from {file_path}...\")\n",
    "    clustering_results = pd.read_csv(file_path)\n",
    "    return clustering_results\n",
    "\n",
    "def load_tremor_data(file_path='filtered_tremor_data.csv'):\n",
    "    \"\"\"\n",
    "    Load filtered tremor data from CSV file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the filtered tremor data CSV file\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Loaded tremor data\n",
    "    \"\"\"\n",
    "    print(f\"Loading tremor data from {file_path}...\")\n",
    "    tremor_data = pd.read_csv(file_path)\n",
    "    tremor_data['starttime'] = pd.to_datetime(tremor_data['starttime'])\n",
    "    return tremor_data\n",
    "\n",
    "def create_cluster_comparison_heatmap(clustering_results):\n",
    "    \"\"\"\n",
    "    Create a heatmap comparing cluster characteristics.\n",
    "    \n",
    "    Args:\n",
    "        clustering_results (pandas.DataFrame): Clustering results\n",
    "    \"\"\"\n",
    "    # Group by cluster and calculate statistics\n",
    "    cluster_stats = clustering_results.groupby('hierarchical_cluster').agg({\n",
    "        'avg_migration_speed': ['mean', 'std', 'min', 'max'],\n",
    "        'avg_migration_direction': ['mean', 'std', 'min', 'max'],\n",
    "        'event_count': ['mean', 'std', 'min', 'max']\n",
    "    })\n",
    "    \n",
    "    # Flatten the multi-level column names\n",
    "    cluster_stats.columns = ['_'.join(col) for col in cluster_stats.columns]\n",
    "    \n",
    "    # Extract the mean values for heatmap\n",
    "    heatmap_data = cluster_stats[[\n",
    "        'avg_migration_speed_mean', \n",
    "        'avg_migration_direction_mean', \n",
    "        'event_count_mean'\n",
    "    ]]\n",
    "    \n",
    "    # Normalize the data for better visualization\n",
    "    normalized_data = (heatmap_data - heatmap_data.min()) / (heatmap_data.max() - heatmap_data.min())\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(normalized_data.T, cmap='viridis', aspect='auto')\n",
    "    \n",
    "    # Add labels\n",
    "    plt.yticks(range(len(normalized_data.columns)), [\n",
    "        'Migration Speed', \n",
    "        'Migration Direction', \n",
    "        'Event Count'\n",
    "    ])\n",
    "    plt.xticks(range(len(normalized_data)), [f'Cluster {i}' for i in normalized_data.index])\n",
    "    \n",
    "    # Add values in cells\n",
    "    for i in range(len(normalized_data)):\n",
    "        for j in range(len(normalized_data.columns)):\n",
    "            plt.text(\n",
    "                i, j, \n",
    "                f\"{heatmap_data.iloc[i, j]:.2f}\", \n",
    "                ha=\"center\", va=\"center\", \n",
    "                color=\"white\" if normalized_data.iloc[i, j] > 0.5 else \"black\"\n",
    "            )\n",
    "    \n",
    "    plt.colorbar(label='Normalized Value')\n",
    "    plt.title('Cluster Characteristics Comparison')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('enhanced_visualizations/cluster_comparison_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def create_3d_cluster_visualization(clustering_results):\n",
    "    \"\"\"\n",
    "    Create a 3D visualization of clusters based on migration parameters.\n",
    "    \n",
    "    Args:\n",
    "        clustering_results (pandas.DataFrame): Clustering results\n",
    "    \"\"\"\n",
    "    # Create 3D plot\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Get unique clusters\n",
    "    clusters = sorted(clustering_results['hierarchical_cluster'].unique())\n",
    "    \n",
    "    # Create a color map\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(clusters)))\n",
    "    \n",
    "    # Plot each cluster\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        cluster_data = clustering_results[clustering_results['hierarchical_cluster'] == cluster]\n",
    "        \n",
    "        # Plot 3D scatter\n",
    "        scatter = ax.scatter(\n",
    "            cluster_data['avg_migration_speed'],\n",
    "            cluster_data['avg_migration_direction'],\n",
    "            cluster_data['event_count'],\n",
    "            s=80,\n",
    "            color=colors[i],\n",
    "            alpha=0.7,\n",
    "            label=f'Cluster {cluster}'\n",
    "        )\n",
    "        \n",
    "        # Add labels for each point\n",
    "        for _, row in cluster_data.iterrows():\n",
    "            ax.text(\n",
    "                row['avg_migration_speed'],\n",
    "                row['avg_migration_direction'],\n",
    "                row['event_count'],\n",
    "                row['peak_label'],\n",
    "                fontsize=8\n",
    "            )\n",
    "    \n",
    "    ax.set_xlabel('Migration Speed (km/h)')\n",
    "    ax.set_ylabel('Migration Direction (degrees)')\n",
    "    ax.set_zlabel('Event Count')\n",
    "    ax.set_title('3D Visualization of Clusters')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('enhanced_visualizations/3d_cluster_visualization.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_cluster_radar_chart(clustering_results):\n",
    "    \"\"\"\n",
    "    Create radar charts for each cluster showing key characteristics.\n",
    "    \n",
    "    Args:\n",
    "        clustering_results (pandas.DataFrame): Clustering results\n",
    "    \"\"\"\n",
    "    # Group by cluster and calculate statistics\n",
    "    cluster_stats = clustering_results.groupby('hierarchical_cluster').agg({\n",
    "        'avg_migration_speed': 'mean',\n",
    "        'avg_migration_direction': 'mean',\n",
    "        'event_count': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Normalize the data for radar chart\n",
    "    normalized_stats = (cluster_stats - cluster_stats.min()) / (cluster_stats.max() - cluster_stats.min())\n",
    "    \n",
    "    # Define categories for radar chart\n",
    "    categories = ['Migration Speed', 'Migration Direction', 'Event Count']\n",
    "    \n",
    "    # Get unique clusters\n",
    "    clusters = sorted(cluster_stats.index)\n",
    "    \n",
    "    # Create a color map\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(clusters)))\n",
    "    \n",
    "    # Create radar chart\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot each cluster as a separate radar chart\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        # Create subplot in a grid\n",
    "        ax = fig.add_subplot(2, (len(clusters) + 1) // 2, i + 1, polar=True)\n",
    "        \n",
    "        # Get normalized values for this cluster\n",
    "        values = normalized_stats.loc[cluster].values\n",
    "        \n",
    "        # Close the loop by repeating the first value\n",
    "        values = np.append(values, values[0])\n",
    "        \n",
    "        # Set the angles for each category (evenly spaced)\n",
    "        angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False)\n",
    "        \n",
    "        # Close the loop by repeating the first angle\n",
    "        angles = np.append(angles, angles[0])\n",
    "        \n",
    "        # Plot the radar chart\n",
    "        ax.plot(angles, values, color=colors[i], linewidth=2)\n",
    "        ax.fill(angles, values, color=colors[i], alpha=0.25)\n",
    "        \n",
    "        # Set the labels\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(categories)\n",
    "        \n",
    "        # Set title\n",
    "        ax.set_title(f'Cluster {cluster}', size=11)\n",
    "        \n",
    "        # Add actual values as text\n",
    "        for j, category in enumerate(categories):\n",
    "            value = cluster_stats.loc[cluster][j]\n",
    "            ax.annotate(\n",
    "                f'{value:.2f}',\n",
    "                (angles[j], values[j]),\n",
    "                xytext=(10, 10),\n",
    "                textcoords='offset points',\n",
    "                fontsize=8\n",
    "            )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('enhanced_visualizations/cluster_radar_charts.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_cluster_map_comparison(clustering_results, tremor_data):\n",
    "    \"\"\"\n",
    "    Create maps comparing spatial distribution of events across clusters.\n",
    "    \n",
    "    Args:\n",
    "        clustering_results (pandas.DataFrame): Clustering results\n",
    "        tremor_data (pandas.DataFrame): Filtered tremor data\n",
    "    \"\"\"\n",
    "    # Get unique clusters\n",
    "    clusters = sorted(clustering_results['hierarchical_cluster'].unique())\n",
    "    \n",
    "    # Create a figure with subplots for each cluster\n",
    "    fig, axes = plt.subplots(1, len(clusters), figsize=(6*len(clusters), 6))\n",
    "    \n",
    "    # Handle case with only one cluster\n",
    "    if len(clusters) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Create a custom colormap for time progression\n",
    "    colors = [(0, 0, 0.8), (0, 0.8, 0), (0.8, 0.8, 0), (0.8, 0, 0)]  # Blue -> Green -> Yellow -> Red\n",
    "    cmap_name = 'time_progression'\n",
    "    cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=100)\n",
    "    \n",
    "    # Plot each cluster\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        ax = axes[i]\n",
    "        cluster_data = clustering_results[clustering_results['hierarchical_cluster'] == cluster]\n",
    "        \n",
    "        # Combine all events from peaks in this cluster\n",
    "        all_cluster_events = pd.DataFrame()\n",
    "        \n",
    "        for _, row in cluster_data.iterrows():\n",
    "            peak_label = row['peak_label']\n",
    "            peak_date = pd.to_datetime(peak_label)\n",
    "            peak_end = peak_date + pd.Timedelta(days=7)\n",
    "            \n",
    "            # Extract events for this peak\n",
    "            peak_events = tremor_data[(tremor_data['starttime'] >= peak_date) & \n",
    "                                     (tremor_data['starttime'] < peak_end)]\n",
    "            \n",
    "            all_cluster_events = pd.concat([all_cluster_events, peak_events])\n",
    "        \n",
    "        # Plot events on map\n",
    "        scatter = ax.scatter(\n",
    "            all_cluster_events['lon'],\n",
    "            all_cluster_events['lat'],\n",
    "            c=range(len(all_cluster_events)),\n",
    "            cmap=cm,\n",
    "            alpha=0.5,\n",
    "            s=30\n",
    "        )\n",
    "        ax.set_facecolor('#f2f2f2')\n",
    "        \n",
    "        # Set labels and title\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        ax.set_title(f'Cluster {cluster} Events\\n({len(all_cluster_events)} events from {len(cluster_data)} peaks)')\n",
    "        \n",
    "        # Add text with cluster statistics\n",
    "        textstr = (\n",
    "            f'Avg Speed: {cluster_data[\"avg_migration_speed\"].mean():.2f} km/h\\n'\n",
    "            f'Avg Direction: {cluster_data[\"avg_migration_direction\"].mean():.2f}°\\n'\n",
    "            f'Avg Events: {cluster_data[\"event_count\"].mean():.0f}'\n",
    "        )\n",
    "        \n",
    "        props = dict(boxstyle='round', facecolor='white', alpha=0.7)\n",
    "        ax.text(0.05, 0.05, textstr, transform=ax.transAxes, fontsize=10,\n",
    "                verticalalignment='bottom', bbox=props)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('enhanced_visualizations/cluster_map_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_cluster_migration_vectors(clustering_results, tremor_data):\n",
    "    \"\"\"\n",
    "    Create visualizations showing migration vectors for each cluster.\n",
    "    \n",
    "    Args:\n",
    "        clustering_results (pandas.DataFrame): Clustering results\n",
    "        tremor_data (pandas.DataFrame): Filtered tremor data\n",
    "    \"\"\"\n",
    "    # Get unique clusters\n",
    "    clusters = sorted(clustering_results['hierarchical_cluster'].unique())\n",
    "    \n",
    "    # Create a figure with subplots for each cluster\n",
    "    fig, axes = plt.subplots(1, len(clusters), figsize=(6*len(clusters), 6))\n",
    "    \n",
    "    # Handle case with only one cluster\n",
    "    if len(clusters) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Plot each cluster\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        ax = axes[i]\n",
    "        cluster_data = clustering_results[clustering_results['hierarchical_cluster'] == cluster]\n",
    "        \n",
    "        # Process each peak in this cluster\n",
    "        for _, row in cluster_data.iterrows():\n",
    "            peak_label = row['peak_label']\n",
    "            peak_date = pd.to_datetime(peak_label)\n",
    "            peak_end = peak_date + pd.Timedelta(days=7)\n",
    "            \n",
    "            # Extract events for this peak\n",
    "            peak_events = tremor_data[(tremor_data['starttime'] >= peak_date) & \n",
    "                                     (tremor_data['starttime'] < peak_end)]\n",
    "            \n",
    "            # Sort by starttime\n",
    "            peak_events = peak_events.sort_values('starttime')\n",
    "            \n",
    "            if len(peak_events) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Get first and last event\n",
    "            first_event = peak_events.iloc[0]\n",
    "            last_event = peak_events.iloc[-1]\n",
    "            \n",
    "            # Plot migration vector\n",
    "            ax.arrow(\n",
    "                first_event['lon'], first_event['lat'],\n",
    "                last_event['lon'] - first_event['lon'],\n",
    "                last_event['lat'] - first_event['lat'],\n",
    "                head_width=0.02, head_length=0.03, fc='blue', ec='blue',\n",
    "                length_includes_head=True, alpha=0.6\n",
    "            )\n",
    "            \n",
    "            # Add label for the peak\n",
    "            ax.annotate(\n",
    "                peak_label,\n",
    "                ((first_event['lon'] + last_event['lon'])/2, \n",
    "                 (first_event['lat'] + last_event['lat'])/2),\n",
    "                fontsize=8\n",
    "            )\n",
    "        \n",
    "        ax.set_facecolor('#f2f2f2')\n",
    "        \n",
    "        # Set labels and title\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        ax.set_title(f'Cluster {cluster} Migration Vectors\\n({len(cluster_data)} peaks)')\n",
    "        \n",
    "        # Add text with cluster statistics\n",
    "        textstr = (\n",
    "            f'Avg Speed: {cluster_data[\"avg_migration_speed\"].mean():.2f} km/h\\n'\n",
    "            f'Avg Direction: {cluster_data[\"avg_migration_direction\"].mean():.2f}°\\n'\n",
    "            f'Avg Events: {cluster_data[\"event_count\"].mean():.0f}'\n",
    "        )\n",
    "        \n",
    "        props = dict(boxstyle='round', facecolor='white', alpha=0.7)\n",
    "        ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "                verticalalignment='top', bbox=props)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('enhanced_visualizations/cluster_migration_vectors.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_cluster_event_density_comparison(clustering_results, tremor_data):\n",
    "    \"\"\"\n",
    "    Create density heatmaps comparing event distribution across clusters.\n",
    "    \n",
    "    Args:\n",
    "        clustering_results (pandas.DataFrame): Clustering results\n",
    "        tremor_data (pandas.DataFrame): Filtered tremor data\n",
    "    \"\"\"\n",
    "    # Get unique clusters\n",
    "    clusters = sorted(clustering_results['hierarchical_cluster'].unique())\n",
    "    \n",
    "    # Create a figure with subplots for each cluster\n",
    "    fig, axes = plt.subplots(1, len(clusters), figsize=(6*len(clusters), 6))\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    # Handle case with only one cluster\n",
    "    if len(clusters) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Plot each cluster\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        ax = axes[i]\n",
    "        cluster_data = clustering_results[clustering_results['hierarchical_cluster'] == cluster]\n",
    "        \n",
    "        # Combine all events from peaks in this cluster\n",
    "        all_cluster_events = pd.DataFrame()\n",
    "        \n",
    "        for _, row in cluster_data.iterrows():\n",
    "            peak_label = row['peak_label']\n",
    "            peak_date = pd.to_datetime(peak_label)\n",
    "            peak_end = peak_date + pd.Timedelta(days=7)\n",
    "            \n",
    "            # Extract events for this peak\n",
    "            peak_events = tremor_data[(tremor_data['starttime'] >= peak_date) & \n",
    "                                     (tremor_data['starttime'] < peak_end)]\n",
    "            \n",
    "            all_cluster_events = pd.concat([all_cluster_events, peak_events])\n",
    "        \n",
    "        if len(all_cluster_events) < 10:\n",
    "            ax.text(0.5, 0.5, \"Insufficient data\", ha='center', va='center')\n",
    "            continue\n",
    "        \n",
    "        # Create a 2D histogram (density map)\n",
    "        lon_bins = np.linspace(all_cluster_events['lon'].min() - 0.01, all_cluster_events['lon'].max() + 0.01, 50)\n",
    "        lat_bins = np.linspace(all_cluster_events['lat'].min() - 0.01, all_cluster_events['lat'].max() + 0.01, 50)\n",
    "        \n",
    "        hist, lon_edges, lat_edges = np.histogram2d(\n",
    "            all_cluster_events['lon'], all_cluster_events['lat'], \n",
    "            bins=[lon_bins, lat_bins]\n",
    "        )\n",
    "        \n",
    "        # Plot the heatmap\n",
    "        lon_centers = (lon_edges[:-1] + lon_edges[1:]) / 2\n",
    "        lat_centers = (lat_edges[:-1] + lat_edges[1:]) / 2\n",
    "        lon_mesh, lat_mesh = np.meshgrid(lon_centers, lat_centers)\n",
    "        \n",
    "        heatmap = ax.pcolormesh(lon_mesh, lat_mesh, hist.T, cmap='hot')\n",
    "        \n",
    "        # Set labels and title\n",
    "        ax.set_xlabel('Longitude',fontsize=15)\n",
    "        ax.set_ylabel('Latitude',fontsize=15)\n",
    "        ax.set_title(f'Cluster {cluster} Event Density\\n({len(all_cluster_events)} events from {len(cluster_data)} peaks)')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "        # Add colorbar\n",
    "        plt.colorbar(heatmap, ax=ax, label='Event Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('enhanced_visualizations/cluster_density_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff0c38d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading clustering results from clustering_results/hierarchical_clustering_results.csv...\n",
      "Loading tremor data from filtered_tremor_data.csv...\n",
      "Creating enhanced visualizations for hierarchical clustering results...\n",
      "Enhanced visualizations complete. Results saved to enhanced_visualizations directory.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Main function to create enhanced visualizations for hierarchical clustering results.\"\"\"\n",
    "# Check if a file path was provided as a command-line argument\n",
    "clustering_results_path = 'clustering_results/hierarchical_clustering_results.csv'\n",
    "\n",
    "# Load clustering results\n",
    "clustering_results = load_clustering_results(clustering_results_path)\n",
    "\n",
    "\n",
    "tremor_data = load_tremor_data()\n",
    "\n",
    "\n",
    "print(\"Creating enhanced visualizations for hierarchical clustering results...\")\n",
    "\n",
    "# Create various visualizations\n",
    "create_cluster_comparison_heatmap(clustering_results)\n",
    "create_3d_cluster_visualization(clustering_results)\n",
    "create_cluster_radar_chart(clustering_results)\n",
    "create_cluster_map_comparison(clustering_results, tremor_data)\n",
    "create_cluster_migration_vectors(clustering_results, tremor_data)\n",
    "create_cluster_event_density_comparison(clustering_results, tremor_data)\n",
    "\n",
    "print(\"Enhanced visualizations complete. Results saved to enhanced_visualizations directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a12307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismk",
   "language": "python",
   "name": "seismk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
